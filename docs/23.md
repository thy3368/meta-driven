# 两地三中心高可用架构方案

## 概述

两地三中心是金融级高可用架构的标准方案，通过在两个城市部署三个数据中心，实现业务连续性和灾难恢复能力。本文档描述 meta-driven 项目的两地三中心架构设计与实施方案。

## 目录

1. [架构设计](#架构设计)
2. [网络架构](#网络架构)
3. [数据同步策略](#数据同步策略)
4. [故障切换方案](#故障切换方案)
5. [Kubernetes 多集群部署](#kubernetes-多集群部署)
6. [数据库高可用](#数据库高可用)
7. [消息队列跨中心同步](#消息队列跨中心同步)
8. [监控与告警](#监控与告警)
9. [演练与验证](#演练与验证)
10. [低时延优化](#低时延优化)

---

## 架构设计

### 整体架构图

```
                                    ┌─────────────────────────────────────┐
                                    │          全局负载均衡 (GSLB)         │
                                    │      DNS智能解析 / Anycast IP        │
                                    └──────────────┬──────────────────────┘
                                                   │
                    ┌──────────────────────────────┼──────────────────────────────┐
                    │                              │                              │
                    ▼                              ▼                              ▼
    ┌───────────────────────────┐  ┌───────────────────────────┐  ┌───────────────────────────┐
    │      北京主中心 (DC1)      │  │     北京同城灾备 (DC2)     │  │     上海异地灾备 (DC3)     │
    │      Primary Center       │  │     Same-City Standby     │  │    Remote DR Center       │
    │                           │  │                           │  │                           │
    │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │
    │  │   K8s Cluster 1     │  │  │  │   K8s Cluster 2     │  │  │  │   K8s Cluster 3     │  │
    │  │   (Active)          │  │  │  │   (Hot Standby)     │  │  │  │   (Warm Standby)    │  │
    │  └─────────────────────┘  │  │  └─────────────────────┘  │  │  └─────────────────────┘  │
    │                           │  │                           │  │                           │
    │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │
    │  │   PostgreSQL        │  │  │  │   PostgreSQL        │  │  │  │   PostgreSQL        │  │
    │  │   (Primary)         │◀─┼──┼─▶│   (Sync Replica)    │◀─┼──┼─▶│   (Async Replica)   │  │
    │  └─────────────────────┘  │  │  └─────────────────────┘  │  │  └─────────────────────┘  │
    │                           │  │                           │  │                           │
    │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │
    │  │   Redis Cluster     │  │  │  │   Redis Cluster     │  │  │  │   Redis Cluster     │  │
    │  │   (Master)          │◀─┼──┼─▶│   (Replica)         │◀─┼──┼─▶│   (Replica)         │  │
    │  └─────────────────────┘  │  │  └─────────────────────┘  │  │  └─────────────────────┘  │
    │                           │  │                           │  │                           │
    │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │  │  ┌─────────────────────┐  │
    │  │   Kafka Cluster     │  │  │  │   Kafka Cluster     │  │  │  │   Kafka Cluster     │  │
    │  │   (Leader)          │◀─┼──┼─▶│   (MirrorMaker)     │◀─┼──┼─▶│   (MirrorMaker)     │  │
    │  └─────────────────────┘  │  │  └─────────────────────┘  │  │  └─────────────────────┘  │
    │                           │  │                           │  │                           │
    └───────────────────────────┘  └───────────────────────────┘  └───────────────────────────┘
              │                              │                              │
              │         同城专线              │         异地专线              │
              │         RTT < 2ms            │         RTT < 30ms           │
              └──────────────────────────────┴──────────────────────────────┘
```

### 中心定位

| 数据中心 | 角色 | 位置 | 网络延迟 | 数据同步 | 切换时间 |
|---------|------|------|---------|---------|---------|
| DC1 北京主中心 | Active | 北京亦庄 | - | Primary | - |
| DC2 北京同城灾备 | Hot Standby | 北京顺义 | < 2ms | 同步复制 | < 30s |
| DC3 上海异地灾备 | Warm Standby | 上海浦东 | < 30ms | 异步复制 | < 5min |

### RPO/RTO 目标

| 场景 | RPO (数据丢失) | RTO (恢复时间) |
|------|---------------|---------------|
| 单节点故障 | 0 | < 10s |
| DC1 整体故障 | 0 | < 30s |
| 北京区域故障 | < 1min | < 5min |
| 全局切换演练 | 0 | < 3min |

---

## 网络架构

### 网络拓扑

```
                                  ┌─────────────────────────────┐
                                  │        互联网接入           │
                                  │    (BGP多线 / Anycast)      │
                                  └─────────────┬───────────────┘
                                                │
                    ┌───────────────────────────┼───────────────────────────┐
                    │                           │                           │
                    ▼                           ▼                           ▼
         ┌──────────────────┐        ┌──────────────────┐        ┌──────────────────┐
         │  北京主中心网络   │        │ 北京同城灾备网络  │        │ 上海异地灾备网络  │
         │                  │        │                  │        │                  │
         │ ┌──────────────┐ │        │ ┌──────────────┐ │        │ ┌──────────────┐ │
         │ │ 边界路由器    │ │        │ │ 边界路由器    │ │        │ │ 边界路由器    │ │
         │ │ (BGP AS)     │ │        │ │ (BGP AS)     │ │        │ │ (BGP AS)     │ │
         │ └──────┬───────┘ │        │ └──────┬───────┘ │        │ └──────┬───────┘ │
         │        │         │        │        │         │        │        │         │
         │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │
         │ │ 防火墙集群    │ │        │ │ 防火墙集群    │ │        │ │ 防火墙集群    │ │
         │ └──────┬───────┘ │        │ └──────┬───────┘ │        │ └──────┬───────┘ │
         │        │         │        │        │         │        │        │         │
         │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │
         │ │ 负载均衡器    │ │        │ │ 负载均衡器    │ │        │ │ 负载均衡器    │ │
         │ │ (L4/L7 LB)   │ │        │ │ (L4/L7 LB)   │ │        │ │ (L4/L7 LB)   │ │
         │ └──────┬───────┘ │        │ └──────┬───────┘ │        │ └──────┬───────┘ │
         │        │         │        │        │         │        │        │         │
         │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │        │ ┌──────▼───────┐ │
         │ │ 核心交换机    │ │        │ │ 核心交换机    │ │        │ │ 核心交换机    │ │
         │ │ (VXLAN)      │ │        │ │ (VXLAN)      │ │        │ │ (VXLAN)      │ │
         │ └──────────────┘ │        │ └──────────────┘ │        │ └──────────────┘ │
         │                  │        │                  │        │                  │
         └────────┬─────────┘        └────────┬─────────┘        └────────┬─────────┘
                  │                           │                           │
                  │     ┌─────────────────────┴─────────────────────┐     │
                  │     │                                           │     │
                  └─────┤          跨中心专线网络                    ├─────┘
                        │     MPLS VPN / SD-WAN 双链路冗余           │
                        │     北京-北京: 10Gbps × 2                  │
                        │     北京-上海: 10Gbps × 2                  │
                        └───────────────────────────────────────────┘
```

### IP 地址规划

```yaml
# 网络地址规划
networks:
  dc1_beijing_primary:
    management: 10.1.0.0/16      # 管理网络
    business: 10.11.0.0/16       # 业务网络
    storage: 10.21.0.0/16        # 存储网络
    pod_cidr: 10.101.0.0/16      # K8s Pod网络
    service_cidr: 10.201.0.0/16  # K8s Service网络

  dc2_beijing_standby:
    management: 10.2.0.0/16
    business: 10.12.0.0/16
    storage: 10.22.0.0/16
    pod_cidr: 10.102.0.0/16
    service_cidr: 10.202.0.0/16

  dc3_shanghai_dr:
    management: 10.3.0.0/16
    business: 10.13.0.0/16
    storage: 10.23.0.0/16
    pod_cidr: 10.103.0.0/16
    service_cidr: 10.203.0.0/16

  # 全局服务 VIP
  global_vip:
    api_gateway: 172.16.1.1/32
    database: 172.16.2.1/32
    cache: 172.16.3.1/32
```

### GSLB 全局负载均衡配置

```yaml
# GSLB DNS 配置
gslb:
  domain: api.meta-driven.example.com

  health_check:
    protocol: https
    path: /actuator/health
    interval: 5s
    timeout: 3s
    threshold: 3

  policies:
    # 默认策略：地理位置优先
    default:
      type: geolocation
      fallback: round_robin

    # 北京用户路由
    beijing:
      primary: dc1_beijing_primary
      secondary: dc2_beijing_standby
      tertiary: dc3_shanghai_dr

    # 上海用户路由
    shanghai:
      primary: dc3_shanghai_dr
      secondary: dc1_beijing_primary
      tertiary: dc2_beijing_standby

  endpoints:
    dc1_beijing_primary:
      ip: 1.1.1.1
      weight: 100
      priority: 1

    dc2_beijing_standby:
      ip: 1.1.1.2
      weight: 50
      priority: 2

    dc3_shanghai_dr:
      ip: 2.2.2.1
      weight: 30
      priority: 3
```

---

## 数据同步策略

### 数据分类与同步策略

| 数据类型 | 一致性要求 | 同步策略 | 延迟容忍 |
|---------|-----------|---------|---------|
| 交易数据 | 强一致性 | 同步复制 | < 10ms |
| 用户数据 | 强一致性 | 同步复制 | < 10ms |
| 配置数据 | 最终一致 | 异步复制 | < 1s |
| 日志数据 | 最终一致 | 异步复制 | < 30s |
| 缓存数据 | 最终一致 | 异步复制 | < 1s |

### 同步架构

```
                    DC1 (Primary)                    DC2 (Same-City)              DC3 (Remote)
                    ┌─────────────┐                  ┌─────────────┐              ┌─────────────┐
                    │             │                  │             │              │             │
     Write ───────▶ │  PostgreSQL │ ──同步复制──────▶ │  PostgreSQL │ ──异步复制──▶ │  PostgreSQL │
                    │   Primary   │     (RTT<2ms)    │   Standby   │   (RTT<30ms) │   Standby   │
                    │             │                  │             │              │             │
                    └──────┬──────┘                  └──────┬──────┘              └──────┬──────┘
                           │                                │                            │
                    Commit │                         Apply  │                     Apply  │
                    Return │                         Logs   │                     Logs   │
                           │                                │                            │
                           ▼                                ▼                            ▼
                    ┌─────────────┐                  ┌─────────────┐              ┌─────────────┐
                    │   Redis     │ ──CRDT复制─────▶ │   Redis     │ ──CRDT复制─▶ │   Redis     │
                    │   Master    │                  │   Replica   │              │   Replica   │
                    └─────────────┘                  └─────────────┘              └─────────────┘
                           │                                │                            │
                           ▼                                ▼                            ▼
                    ┌─────────────┐                  ┌─────────────┐              ┌─────────────┐
                    │   Kafka     │ ──MirrorMaker──▶ │   Kafka     │ ──MM2─────▶  │   Kafka     │
                    │   Cluster   │                  │   Cluster   │              │   Cluster   │
                    └─────────────┘                  └─────────────┘              └─────────────┘
```

### 数据库同步配置

```sql
-- PostgreSQL 主库配置 (DC1)
-- postgresql.conf

# 流复制配置
wal_level = replica
max_wal_senders = 10
wal_keep_size = 1GB
max_replication_slots = 10

# 同步复制配置 (DC2)
synchronous_standby_names = 'FIRST 1 (dc2_standby)'
synchronous_commit = remote_apply

# 异步复制配置 (DC3) - 自动fallback
# DC3使用异步复制，不在synchronous_standby_names中
```

```sql
-- pg_hba.conf 复制授权

# DC2 同城灾备 - 同步复制
host replication replicator 10.12.0.0/16 scram-sha-256

# DC3 异地灾备 - 异步复制
host replication replicator 10.13.0.0/16 scram-sha-256
```

```bash
#!/bin/bash
# scripts/setup-replication.sh
# PostgreSQL 复制配置脚本

set -euo pipefail

# DC2 同步备库配置
setup_dc2_standby() {
    cat > /var/lib/postgresql/data/postgresql.auto.conf << EOF
primary_conninfo = 'host=10.11.1.1 port=5432 user=replicator password=xxx application_name=dc2_standby'
primary_slot_name = 'dc2_slot'
recovery_target_timeline = 'latest'
EOF
    touch /var/lib/postgresql/data/standby.signal
}

# DC3 异步备库配置
setup_dc3_standby() {
    cat > /var/lib/postgresql/data/postgresql.auto.conf << EOF
primary_conninfo = 'host=10.11.1.1 port=5432 user=replicator password=xxx application_name=dc3_standby'
primary_slot_name = 'dc3_slot'
recovery_target_timeline = 'latest'
EOF
    touch /var/lib/postgresql/data/standby.signal
}
```

---

## 故障切换方案

### 故障场景分类

```
故障场景
├── 级别1: 单节点故障
│   ├── 应用Pod故障 → K8s自动重建
│   ├── 数据库从节点故障 → 自动剔除
│   └── 缓存节点故障 → 自动故障转移
│
├── 级别2: 单中心部分故障
│   ├── 机架故障 → 反亲和性自动恢复
│   └── 网络分区 → GSLB自动切换
│
├── 级别3: 单中心整体故障 (DC1故障)
│   ├── 检测时间: < 10s
│   ├── 切换目标: DC2
│   ├── 数据丢失: 0 (同步复制)
│   └── 恢复时间: < 30s
│
└── 级别4: 区域性灾难 (北京区域故障)
    ├── 检测时间: < 30s
    ├── 切换目标: DC3
    ├── 数据丢失: < 1min (异步复制)
    └── 恢复时间: < 5min
```

### 自动故障切换流程

```
                    ┌─────────────────────────────────────────┐
                    │           监控系统检测故障              │
                    │     Prometheus + 自定义探针              │
                    └─────────────────┬───────────────────────┘
                                      │
                                      ▼
                    ┌─────────────────────────────────────────┐
                    │           故障确认 (多点校验)            │
                    │  • 健康检查连续失败 ≥ 3次               │
                    │  • 多个监控点确认                       │
                    │  • 排除网络抖动                         │
                    └─────────────────┬───────────────────────┘
                                      │
                    ┌─────────────────┴─────────────────┐
                    │                                   │
                    ▼                                   ▼
         ┌──────────────────┐               ┌──────────────────┐
         │   DC1→DC2 切换    │               │  北京→上海 切换   │
         │   (同城灾备)       │               │   (异地灾备)      │
         └────────┬─────────┘               └────────┬─────────┘
                  │                                   │
                  ▼                                   ▼
    ┌─────────────────────────┐       ┌─────────────────────────┐
    │ 1. GSLB切换DNS          │       │ 1. GSLB切换DNS          │
    │ 2. 数据库提升DC2为主     │       │ 2. 数据库提升DC3为主     │
    │ 3. 应用切换数据源        │       │ 3. 应用切换数据源        │
    │ 4. 验证服务可用性        │       │ 4. 验证服务可用性        │
    └─────────────────────────┘       └─────────────────────────┘
                  │                                   │
                  ▼                                   ▼
    ┌─────────────────────────┐       ┌─────────────────────────┐
    │ RTO < 30s, RPO = 0      │       │ RTO < 5min, RPO < 1min  │
    └─────────────────────────┘       └─────────────────────────┘
```

### 自动故障切换脚本

```bash
#!/bin/bash
# scripts/failover-controller.sh
# 两地三中心故障切换控制器

set -euo pipefail

# 配置
PROMETHEUS_URL="${PROMETHEUS_URL:-http://prometheus:9090}"
ALERTMANAGER_URL="${ALERTMANAGER_URL:-http://alertmanager:9093}"

DC1_API="https://dc1-api.example.com"
DC2_API="https://dc2-api.example.com"
DC3_API="https://dc3-api.example.com"

# 日志函数
log_info() { echo "[INFO] $(date '+%Y-%m-%d %H:%M:%S') $1"; }
log_warn() { echo "[WARN] $(date '+%Y-%m-%d %H:%M:%S') $1"; }
log_error() { echo "[ERROR] $(date '+%Y-%m-%d %H:%M:%S') $1"; }

# 健康检查
check_dc_health() {
    local dc=$1
    local api_url=$2
    local max_retries=3
    local retry=0

    while [ $retry -lt $max_retries ]; do
        if curl -sf "${api_url}/actuator/health" --connect-timeout 5 > /dev/null 2>&1; then
            return 0
        fi
        retry=$((retry + 1))
        sleep 2
    done

    return 1
}

# 获取当前主中心
get_active_dc() {
    # 从GSLB或配置中心获取当前活跃DC
    curl -sf "http://config-center/api/active-dc" | jq -r '.dc'
}

# 数据库故障切换
failover_database() {
    local target_dc=$1
    log_info "执行数据库故障切换到 ${target_dc}"

    case $target_dc in
        dc2)
            # DC2 提升为主库
            kubectl exec -n database pg-dc2-0 -- \
                pg_ctl promote -D /var/lib/postgresql/data
            ;;
        dc3)
            # DC3 提升为主库
            kubectl exec -n database pg-dc3-0 -- \
                pg_ctl promote -D /var/lib/postgresql/data
            ;;
    esac

    log_info "数据库故障切换完成"
}

# Redis 故障切换
failover_redis() {
    local target_dc=$1
    log_info "执行 Redis 故障切换到 ${target_dc}"

    # 使用 Redis Sentinel 执行故障切换
    case $target_dc in
        dc2)
            redis-cli -h sentinel-dc2 SENTINEL FAILOVER mymaster
            ;;
        dc3)
            redis-cli -h sentinel-dc3 SENTINEL FAILOVER mymaster
            ;;
    esac

    log_info "Redis 故障切换完成"
}

# GSLB 切换
switch_gslb() {
    local target_dc=$1
    log_info "切换 GSLB 到 ${target_dc}"

    # 更新 GSLB 配置
    curl -X PUT "http://gslb-controller/api/switch" \
        -H "Content-Type: application/json" \
        -d "{\"active_dc\": \"${target_dc}\"}"

    # 等待 DNS TTL 过期
    log_info "等待 DNS 切换生效 (30s)..."
    sleep 30

    log_info "GSLB 切换完成"
}

# 应用配置切换
switch_app_config() {
    local target_dc=$1
    log_info "切换应用配置到 ${target_dc}"

    # 更新 ConfigMap
    kubectl patch configmap app-config -n production \
        --type merge \
        -p "{\"data\":{\"active_dc\":\"${target_dc}\"}}"

    # 滚动重启应用
    kubectl rollout restart deployment/meta-driven -n production

    log_info "应用配置切换完成"
}

# 发送告警
send_alert() {
    local severity=$1
    local message=$2

    curl -X POST "${ALERTMANAGER_URL}/api/v1/alerts" \
        -H "Content-Type: application/json" \
        -d "[{
            \"labels\": {
                \"alertname\": \"DataCenterFailover\",
                \"severity\": \"${severity}\"
            },
            \"annotations\": {
                \"summary\": \"${message}\"
            }
        }]"
}

# 同城故障切换 (DC1 → DC2)
failover_to_dc2() {
    log_warn "开始执行同城故障切换: DC1 → DC2"
    send_alert "critical" "开始执行同城故障切换: DC1 → DC2"

    local start_time=$(date +%s)

    # 1. 数据库切换
    failover_database "dc2"

    # 2. Redis 切换
    failover_redis "dc2"

    # 3. GSLB 切换
    switch_gslb "dc2"

    # 4. 应用配置切换
    switch_app_config "dc2"

    # 5. 验证
    if check_dc_health "dc2" "$DC2_API"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log_info "同城故障切换完成，耗时: ${duration}s"
        send_alert "warning" "同城故障切换完成，耗时: ${duration}s"
    else
        log_error "同城故障切换后 DC2 健康检查失败"
        send_alert "critical" "同城故障切换后 DC2 健康检查失败，需要人工介入"
        exit 1
    fi
}

# 异地故障切换 (北京 → 上海)
failover_to_dc3() {
    log_warn "开始执行异地故障切换: 北京 → 上海"
    send_alert "critical" "开始执行异地故障切换: 北京 → 上海"

    local start_time=$(date +%s)

    # 1. 数据库切换 (异步复制可能有数据丢失)
    log_warn "异地切换将使用异步复制，可能存在数据丢失"
    failover_database "dc3"

    # 2. Redis 切换
    failover_redis "dc3"

    # 3. GSLB 切换
    switch_gslb "dc3"

    # 4. 应用配置切换
    switch_app_config "dc3"

    # 5. 验证
    if check_dc_health "dc3" "$DC3_API"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log_info "异地故障切换完成，耗时: ${duration}s"
        send_alert "warning" "异地故障切换完成，耗时: ${duration}s，请检查数据一致性"
    else
        log_error "异地故障切换后 DC3 健康检查失败"
        send_alert "critical" "异地故障切换后 DC3 健康检查失败，需要人工介入"
        exit 1
    fi
}

# 主监控逻辑
main() {
    log_info "启动两地三中心故障切换控制器..."

    local dc1_failures=0
    local dc2_failures=0
    local max_failures=3

    while true; do
        # 检查 DC1
        if ! check_dc_health "dc1" "$DC1_API"; then
            dc1_failures=$((dc1_failures + 1))
            log_warn "DC1 健康检查失败 (${dc1_failures}/${max_failures})"

            if [ $dc1_failures -ge $max_failures ]; then
                # DC1 故障，检查 DC2
                if check_dc_health "dc2" "$DC2_API"; then
                    failover_to_dc2
                else
                    # DC2 也故障，切换到 DC3
                    failover_to_dc3
                fi
                dc1_failures=0
            fi
        else
            dc1_failures=0
        fi

        sleep 5
    done
}

# 手动切换命令
case "${1:-monitor}" in
    monitor)
        main
        ;;
    failover-dc2)
        failover_to_dc2
        ;;
    failover-dc3)
        failover_to_dc3
        ;;
    status)
        echo "DC1: $(check_dc_health dc1 $DC1_API && echo 'healthy' || echo 'unhealthy')"
        echo "DC2: $(check_dc_health dc2 $DC2_API && echo 'healthy' || echo 'unhealthy')"
        echo "DC3: $(check_dc_health dc3 $DC3_API && echo 'healthy' || echo 'unhealthy')"
        echo "Active DC: $(get_active_dc)"
        ;;
    *)
        echo "Usage: $0 {monitor|failover-dc2|failover-dc3|status}"
        exit 1
        ;;
esac
```

---

## Kubernetes 多集群部署

### 集群架构

```yaml
# 多集群配置
clusters:
  dc1-beijing-primary:
    region: beijing
    zone: az-1
    role: primary
    nodes:
      master: 3
      worker: 10
    resources:
      cpu: 200 cores
      memory: 800 GB
      storage: 50 TB

  dc2-beijing-standby:
    region: beijing
    zone: az-2
    role: hot-standby
    nodes:
      master: 3
      worker: 8
    resources:
      cpu: 160 cores
      memory: 640 GB
      storage: 40 TB

  dc3-shanghai-dr:
    region: shanghai
    zone: az-1
    role: warm-standby
    nodes:
      master: 3
      worker: 5
    resources:
      cpu: 100 cores
      memory: 400 GB
      storage: 30 TB
```

### 多集群部署配置

```yaml
# k8s/multi-cluster/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: meta-driven
  namespace: production
  labels:
    app: meta-driven
    tier: backend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: meta-driven
  template:
    metadata:
      labels:
        app: meta-driven
        version: v1.0.0
    spec:
      affinity:
        # 跨可用区分布
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: meta-driven
            topologyKey: topology.kubernetes.io/zone
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: meta-driven
              topologyKey: kubernetes.io/hostname

      containers:
      - name: app
        image: meta-driven:v1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "prod"
        - name: DATACENTER
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['topology.kubernetes.io/zone']
        - name: JAVA_OPTS
          value: "-XX:+UseG1GC -XX:MaxGCPauseMillis=10 -Xms1g -Xmx2g"

        resources:
          requests:
            memory: "1Gi"
            cpu: "1"
          limits:
            memory: "2Gi"
            cpu: "2"

        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5
          failureThreshold: 3

        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3

      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: meta-driven
```

### 多集群服务发现 (Submariner / Cilium Cluster Mesh)

```yaml
# k8s/multi-cluster/service-export.yaml
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceExport
metadata:
  name: meta-driven
  namespace: production
---
# k8s/multi-cluster/service-import.yaml
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceImport
metadata:
  name: meta-driven
  namespace: production
spec:
  type: ClusterSetIP
  ports:
  - port: 8080
    protocol: TCP
```

### Istio 多集群流量管理

```yaml
# k8s/multi-cluster/istio-destination-rule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: meta-driven
  namespace: production
spec:
  host: meta-driven.production.svc.cluster.local
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1000
      http:
        h2UpgradePolicy: UPGRADE
        http1MaxPendingRequests: 1000
        http2MaxRequests: 1000
    loadBalancer:
      simple: LEAST_REQUEST
      localityLbSetting:
        enabled: true
        failover:
        - from: beijing/az-1
          to: beijing/az-2
        - from: beijing/*
          to: shanghai/az-1
    outlierDetection:
      consecutiveErrors: 5
      interval: 10s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
---
# k8s/multi-cluster/istio-virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: meta-driven
  namespace: production
spec:
  hosts:
  - meta-driven
  http:
  - route:
    - destination:
        host: meta-driven
        subset: dc1
      weight: 80
    - destination:
        host: meta-driven
        subset: dc2
      weight: 15
    - destination:
        host: meta-driven
        subset: dc3
      weight: 5
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure
```

---

## 数据库高可用

### PostgreSQL 高可用架构

```
                DC1 (北京主中心)                  DC2 (北京同城)              DC3 (上海异地)
                ┌─────────────────┐              ┌─────────────────┐         ┌─────────────────┐
                │                 │              │                 │         │                 │
     Write ────▶│  PostgreSQL     │──同步复制───▶│  PostgreSQL     │──异步──▶│  PostgreSQL     │
                │  Primary        │   (2PC)      │  Sync Standby   │  复制   │  Async Standby  │
                │                 │              │                 │         │                 │
                └────────┬────────┘              └────────┬────────┘         └────────┬────────┘
                         │                                │                           │
                         │                                │                           │
                ┌────────▼────────┐              ┌────────▼────────┐         ┌────────▼────────┐
                │   Patroni       │              │   Patroni       │         │   Patroni       │
                │   Controller    │◀────────────▶│   Controller    │◀───────▶│   Controller    │
                └────────┬────────┘              └────────┬────────┘         └────────┬────────┘
                         │                                │                           │
                         └────────────────┬───────────────┴───────────────────────────┘
                                          │
                                 ┌────────▼────────┐
                                 │     etcd        │
                                 │   (Distributed) │
                                 └─────────────────┘
```

### Patroni 配置

```yaml
# patroni-dc1.yml
scope: meta-driven-cluster
name: pg-dc1-primary

restapi:
  listen: 0.0.0.0:8008
  connect_address: pg-dc1-primary:8008

etcd3:
  hosts:
  - etcd-dc1-1:2379
  - etcd-dc1-2:2379
  - etcd-dc1-3:2379
  - etcd-dc2-1:2379
  - etcd-dc2-2:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576  # 1MB
    synchronous_mode: true
    synchronous_mode_strict: true
    synchronous_node_count: 1

    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        wal_level: replica
        hot_standby: "on"
        max_connections: 1000
        max_wal_senders: 10
        max_replication_slots: 10
        wal_keep_size: 1GB
        synchronous_commit: "remote_apply"
        # 低时延优化
        shared_buffers: "8GB"
        effective_cache_size: "24GB"
        maintenance_work_mem: "2GB"
        checkpoint_completion_target: 0.9
        wal_buffers: "64MB"
        default_statistics_target: 100
        random_page_cost: 1.1
        effective_io_concurrency: 200
        work_mem: "64MB"
        min_wal_size: "1GB"
        max_wal_size: "4GB"

  initdb:
  - encoding: UTF8
  - data-checksums

postgresql:
  listen: 0.0.0.0:5432
  connect_address: pg-dc1-primary:5432
  data_dir: /var/lib/postgresql/data
  authentication:
    superuser:
      username: postgres
      password: ${POSTGRES_PASSWORD}
    replication:
      username: replicator
      password: ${REPLICATION_PASSWORD}

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```

### PgBouncer 连接池配置

```ini
# pgbouncer.ini - 低时延连接池配置

[databases]
meta_driven = host=pg-vip port=5432 dbname=meta_driven

[pgbouncer]
listen_addr = 0.0.0.0
listen_port = 6432
auth_type = scram-sha-256
auth_file = /etc/pgbouncer/userlist.txt

# 连接池模式
pool_mode = transaction

# 连接池大小
default_pool_size = 100
min_pool_size = 20
reserve_pool_size = 20

# 超时配置
server_connect_timeout = 5
server_idle_timeout = 600
server_lifetime = 3600
client_idle_timeout = 0
query_timeout = 0

# 低时延优化
tcp_keepalive = 1
tcp_keepidle = 60
tcp_keepintvl = 10
tcp_keepcnt = 6
tcp_user_timeout = 30000

# 日志
log_connections = 0
log_disconnections = 0
log_pooler_errors = 1
```

---

## 消息队列跨中心同步

### Kafka MirrorMaker 2.0 配置

```properties
# mm2-dc1-to-dc2.properties
# DC1 -> DC2 同城复制

clusters = dc1, dc2

dc1.bootstrap.servers = kafka-dc1-1:9092,kafka-dc1-2:9092,kafka-dc1-3:9092
dc2.bootstrap.servers = kafka-dc2-1:9092,kafka-dc2-2:9092,kafka-dc2-3:9092

# 复制配置
dc1->dc2.enabled = true
dc1->dc2.topics = .*
dc1->dc2.groups = .*

# 同步复制延迟目标
replication.factor = 3
sync.topic.configs.enabled = true
sync.topic.acls.enabled = true

# 偏移量同步
emit.checkpoints.enabled = true
emit.checkpoints.interval.seconds = 5
sync.group.offsets.enabled = true
sync.group.offsets.interval.seconds = 5

# 低时延配置
offset.lag.max = 100
refresh.topics.interval.seconds = 30
refresh.groups.interval.seconds = 30

# 生产者配置
producer.acks = all
producer.batch.size = 16384
producer.linger.ms = 0
producer.buffer.memory = 33554432
```

```properties
# mm2-dc1-to-dc3.properties
# DC1 -> DC3 异地复制 (异步)

clusters = dc1, dc3

dc1.bootstrap.servers = kafka-dc1-1:9092,kafka-dc1-2:9092,kafka-dc1-3:9092
dc3.bootstrap.servers = kafka-dc3-1:9092,kafka-dc3-2:9092,kafka-dc3-3:9092

dc1->dc3.enabled = true
dc1->dc3.topics = .*

# 异步复制配置
replication.factor = 3
producer.acks = 1
producer.batch.size = 65536
producer.linger.ms = 100

# 压缩以减少网络带宽
producer.compression.type = lz4
```

### Kafka 跨中心部署

```yaml
# k8s/kafka/statefulset-dc1.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: messaging
spec:
  serviceName: kafka
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
        dc: dc1
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: kafka
            topologyKey: kubernetes.io/hostname

      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.5.0
        ports:
        - containerPort: 9092
          name: client
        - containerPort: 9093
          name: controller
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://:9092,CONTROLLER://:9093"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://$(POD_NAME).kafka.messaging.svc.cluster.local:9092"
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: "0@kafka-0.kafka:9093,1@kafka-1.kafka:9093,2@kafka-2.kafka:9093"
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        # 低时延配置
        - name: KAFKA_NUM_NETWORK_THREADS
          value: "8"
        - name: KAFKA_NUM_IO_THREADS
          value: "16"
        - name: KAFKA_SOCKET_SEND_BUFFER_BYTES
          value: "102400"
        - name: KAFKA_SOCKET_RECEIVE_BUFFER_BYTES
          value: "102400"
        - name: KAFKA_SOCKET_REQUEST_MAX_BYTES
          value: "104857600"

        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"

        volumeMounts:
        - name: data
          mountPath: /var/lib/kafka/data

  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

---

## 监控与告警

### 多中心监控架构

```
                    ┌─────────────────────────────────────────┐
                    │          全局监控中心 (DC1)              │
                    │                                         │
                    │  ┌─────────────┐  ┌─────────────┐       │
                    │  │  Grafana    │  │ Alertmanager│       │
                    │  │  (Global)   │  │  (HA)       │       │
                    │  └──────┬──────┘  └──────┬──────┘       │
                    │         │                │              │
                    │  ┌──────▼────────────────▼──────┐       │
                    │  │      Thanos Query Frontend    │       │
                    │  └──────────────┬───────────────┘       │
                    └─────────────────┼───────────────────────┘
                                      │
           ┌──────────────────────────┼──────────────────────────┐
           │                          │                          │
           ▼                          ▼                          ▼
   ┌───────────────┐          ┌───────────────┐          ┌───────────────┐
   │   DC1 监控     │          │   DC2 监控     │          │   DC3 监控     │
   │               │          │               │          │               │
   │ ┌───────────┐ │          │ ┌───────────┐ │          │ ┌───────────┐ │
   │ │Prometheus │ │          │ │Prometheus │ │          │ │Prometheus │ │
   │ │+ Thanos   │ │          │ │+ Thanos   │ │          │ │+ Thanos   │ │
   │ │Sidecar    │ │          │ │Sidecar    │ │          │ │Sidecar    │ │
   │ └───────────┘ │          │ └───────────┘ │          │ └───────────┘ │
   │               │          │               │          │               │
   │ ┌───────────┐ │          │ ┌───────────┐ │          │ ┌───────────┐ │
   │ │   Loki    │ │          │ │   Loki    │ │          │ │   Loki    │ │
   │ └───────────┘ │          │ └───────────┘ │          │ └───────────┘ │
   │               │          │               │          │               │
   │ ┌───────────┐ │          │ ┌───────────┐ │          │ ┌───────────┐ │
   │ │   Tempo   │ │          │ │   Tempo   │ │          │ │   Tempo   │ │
   │ └───────────┘ │          │ └───────────┘ │          │ └───────────┘ │
   └───────────────┘          └───────────────┘          └───────────────┘
```

### 跨中心告警规则

```yaml
# config/prometheus/multi-dc-alerts.yml
groups:
- name: multi-dc-alerts
  rules:
  # 数据中心健康检查
  - alert: DataCenterUnhealthy
    expr: |
      up{job="dc-health-check"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "数据中心 {{ $labels.dc }} 不可用"
      description: "数据中心健康检查连续失败超过1分钟"

  # 数据同步延迟
  - alert: ReplicationLagHigh
    expr: |
      pg_replication_lag_seconds > 10
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "数据库复制延迟过高"
      description: "{{ $labels.instance }} 复制延迟: {{ $value }}s"

  # 同步复制降级
  - alert: SyncReplicationDegraded
    expr: |
      pg_stat_replication_sync_state{state!="sync"} > 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "同步复制降级"
      description: "数据库同步复制已降级为异步模式"

  # 跨中心网络延迟
  - alert: CrossDCLatencyHigh
    expr: |
      probe_duration_seconds{job="cross-dc-probe"} > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "跨中心网络延迟过高"
      description: "{{ $labels.source }} -> {{ $labels.target }} 延迟: {{ $value }}s"

  # Kafka 复制延迟
  - alert: KafkaMirrorMakerLag
    expr: |
      kafka_consumer_group_lag{group=~"mm2.*"} > 10000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Kafka MirrorMaker 复制延迟"
      description: "MirrorMaker lag: {{ $value }} messages"

  # 故障切换检测
  - alert: FailoverTriggered
    expr: |
      changes(patroni_master{job="patroni"}[5m]) > 0
    for: 0m
    labels:
      severity: critical
    annotations:
      summary: "数据库发生故障切换"
      description: "Patroni 检测到主库切换"

- name: capacity-alerts
  rules:
  # 跨中心容量不均衡
  - alert: CrossDCCapacityImbalance
    expr: |
      abs(
        sum(container_memory_usage_bytes{namespace="production"}) by (dc)
        /
        avg(sum(container_memory_usage_bytes{namespace="production"}) by (dc))
        - 1
      ) > 0.3
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "跨中心容量不均衡"
      description: "数据中心间资源使用差异超过30%"
```

### Grafana 多中心 Dashboard

```json
{
  "dashboard": {
    "title": "两地三中心监控",
    "panels": [
      {
        "title": "数据中心健康状态",
        "type": "stat",
        "gridPos": {"x": 0, "y": 0, "w": 24, "h": 4},
        "targets": [
          {
            "expr": "up{job=\"dc-health-check\"}",
            "legendFormat": "{{dc}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"type": "value", "options": {"0": {"text": "DOWN", "color": "red"}}},
              {"type": "value", "options": {"1": {"text": "UP", "color": "green"}}}
            ]
          }
        }
      },
      {
        "title": "跨中心网络延迟",
        "type": "timeseries",
        "gridPos": {"x": 0, "y": 4, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "probe_duration_seconds{job=\"cross-dc-probe\"} * 1000",
            "legendFormat": "{{source}} -> {{target}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "ms",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 10, "color": "yellow"},
                {"value": 50, "color": "red"}
              ]
            }
          }
        }
      },
      {
        "title": "数据库复制延迟",
        "type": "timeseries",
        "gridPos": {"x": 12, "y": 4, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "pg_replication_lag_seconds",
            "legendFormat": "{{instance}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 1, "color": "yellow"},
                {"value": 10, "color": "red"}
              ]
            }
          }
        }
      },
      {
        "title": "各中心请求分布",
        "type": "piechart",
        "gridPos": {"x": 0, "y": 12, "w": 8, "h": 8},
        "targets": [
          {
            "expr": "sum(rate(http_server_requests_seconds_count[5m])) by (dc)",
            "legendFormat": "{{dc}}"
          }
        ]
      },
      {
        "title": "各中心 P99 延迟对比",
        "type": "timeseries",
        "gridPos": {"x": 8, "y": 12, "w": 16, "h": 8},
        "targets": [
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket[5m])) by (le, dc)) * 1000",
            "legendFormat": "{{dc}} P99"
          }
        ],
        "fieldConfig": {
          "defaults": {"unit": "ms"}
        }
      }
    ]
  }
}
```

---

## 演练与验证

### 演练计划

| 演练类型 | 频率 | 范围 | 预期RTO | 通知 |
|---------|------|------|--------|------|
| 单节点故障模拟 | 每周 | 单个Pod/节点 | < 10s | 仅团队内部 |
| 同城切换演练 | 每月 | DC1 → DC2 | < 30s | 业务通知 |
| 异地切换演练 | 每季度 | 北京 → 上海 | < 5min | 全量通知 |
| 混沌工程测试 | 每周 | 随机故障注入 | 视情况 | 团队内部 |

### 混沌工程配置 (Chaos Mesh)

```yaml
# chaos/pod-failure.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-failure-dc1
  namespace: chaos-testing
spec:
  action: pod-failure
  mode: one
  selector:
    namespaces:
    - production
    labelSelectors:
      app: meta-driven
      dc: dc1
  duration: "60s"
  scheduler:
    cron: "@weekly"
---
# chaos/network-delay.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: cross-dc-delay
  namespace: chaos-testing
spec:
  action: delay
  mode: all
  selector:
    namespaces:
    - production
    labelSelectors:
      dc: dc1
  delay:
    latency: "100ms"
    correlation: "25"
    jitter: "10ms"
  direction: to
  target:
    selector:
      namespaces:
      - production
      labelSelectors:
        dc: dc3
  duration: "5m"
---
# chaos/network-partition.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: dc-partition
  namespace: chaos-testing
spec:
  action: partition
  mode: all
  selector:
    namespaces:
    - production
    labelSelectors:
      dc: dc1
  direction: both
  target:
    selector:
      namespaces:
      - production
      labelSelectors:
        dc: dc2
  duration: "2m"
```

### 演练脚本

```bash
#!/bin/bash
# scripts/dr-drill.sh
# 灾难恢复演练脚本

set -euo pipefail

DRILL_TYPE=${1:-"same-city"}
DRY_RUN=${DRY_RUN:-"true"}

log_info() { echo "[INFO] $(date '+%Y-%m-%d %H:%M:%S') $1"; }
log_warn() { echo "[WARN] $(date '+%Y-%m-%d %H:%M:%S') $1"; }

# 演练前检查
pre_drill_check() {
    log_info "执行演练前检查..."

    # 检查所有中心健康
    for dc in dc1 dc2 dc3; do
        if ! ./scripts/failover-controller.sh status | grep -q "${dc}.*healthy"; then
            log_warn "${dc} 不健康，取消演练"
            exit 1
        fi
    done

    # 检查复制延迟
    local lag=$(curl -s "http://prometheus:9090/api/v1/query?query=max(pg_replication_lag_seconds)" | jq -r '.data.result[0].value[1]')
    if (( $(echo "$lag > 1" | bc -l) )); then
        log_warn "复制延迟过高: ${lag}s，取消演练"
        exit 1
    fi

    log_info "演练前检查通过"
}

# 同城切换演练
same_city_drill() {
    log_info "开始同城切换演练 (DC1 → DC2)"

    local start_time=$(date +%s)

    if [ "$DRY_RUN" = "false" ]; then
        # 执行切换
        ./scripts/failover-controller.sh failover-dc2
    else
        log_info "[DRY RUN] 将执行: ./scripts/failover-controller.sh failover-dc2"
        sleep 5
    fi

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log_info "同城切换完成，耗时: ${duration}s"

    # 验证
    verify_drill "dc2"

    # 切回
    log_info "切回原主中心..."
    if [ "$DRY_RUN" = "false" ]; then
        ./scripts/failover-controller.sh failover-dc1
    fi

    log_info "同城切换演练完成"
}

# 异地切换演练
remote_drill() {
    log_info "开始异地切换演练 (北京 → 上海)"

    local start_time=$(date +%s)

    if [ "$DRY_RUN" = "false" ]; then
        ./scripts/failover-controller.sh failover-dc3
    else
        log_info "[DRY RUN] 将执行: ./scripts/failover-controller.sh failover-dc3"
        sleep 10
    fi

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log_info "异地切换完成，耗时: ${duration}s"

    # 验证
    verify_drill "dc3"

    # 切回
    log_info "切回原主中心..."
    if [ "$DRY_RUN" = "false" ]; then
        ./scripts/failover-controller.sh failover-dc1
    fi

    log_info "异地切换演练完成"
}

# 验证演练结果
verify_drill() {
    local target_dc=$1
    log_info "验证 ${target_dc} 服务状态..."

    # 检查应用健康
    local api_url
    case $target_dc in
        dc1) api_url="https://dc1-api.example.com" ;;
        dc2) api_url="https://dc2-api.example.com" ;;
        dc3) api_url="https://dc3-api.example.com" ;;
    esac

    if curl -sf "${api_url}/actuator/health" > /dev/null; then
        log_info "${target_dc} 应用健康检查通过"
    else
        log_warn "${target_dc} 应用健康检查失败"
    fi

    # 检查数据库
    log_info "验证数据库写入..."
    # 执行写入测试...

    # 检查消息队列
    log_info "验证消息队列..."
    # 执行消息测试...
}

# 生成演练报告
generate_report() {
    local drill_type=$1
    local report_file="drill-report-$(date +%Y%m%d-%H%M%S).md"

    cat > "$report_file" << EOF
# 灾难恢复演练报告

## 基本信息
- 演练类型: ${drill_type}
- 演练时间: $(date '+%Y-%m-%d %H:%M:%S')
- 演练模式: ${DRY_RUN}

## 演练结果
[待填写]

## 问题发现
[待填写]

## 改进建议
[待填写]
EOF

    log_info "演练报告已生成: ${report_file}"
}

# 主函数
main() {
    log_info "========== 灾难恢复演练开始 =========="

    pre_drill_check

    case $DRILL_TYPE in
        same-city)
            same_city_drill
            ;;
        remote)
            remote_drill
            ;;
        full)
            same_city_drill
            remote_drill
            ;;
        *)
            echo "用法: $0 {same-city|remote|full}"
            exit 1
            ;;
    esac

    generate_report "$DRILL_TYPE"

    log_info "========== 灾难恢复演练结束 =========="
}

main
```

---

## 低时延优化

### 跨中心通信优化

```yaml
# 应用配置优化
spring:
  datasource:
    hikari:
      # 连接池优化
      minimumIdle: 20
      maximumPoolSize: 100
      connectionTimeout: 5000
      idleTimeout: 600000
      maxLifetime: 1800000
      # 低时延配置
      leakDetectionThreshold: 30000
      validationTimeout: 3000

  redis:
    # Redis 连接优化
    lettuce:
      pool:
        min-idle: 10
        max-idle: 50
        max-active: 100
      shutdown-timeout: 100ms
    timeout: 1000ms

  kafka:
    producer:
      # 低时延生产者配置
      acks: 1  # 同城使用 all
      batch-size: 16384
      linger-ms: 0
      buffer-memory: 33554432
      compression-type: lz4
    consumer:
      # 低时延消费者配置
      fetch-min-size: 1
      fetch-max-wait: 100
```

### 网络层优化

```bash
# 系统网络参数优化
cat >> /etc/sysctl.conf << EOF
# TCP 低时延优化
net.ipv4.tcp_low_latency = 1
net.ipv4.tcp_nodelay = 1
net.ipv4.tcp_sack = 1
net.ipv4.tcp_timestamps = 1

# 连接优化
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 65535
net.core.netdev_max_backlog = 65535

# 缓冲区优化
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# TIME_WAIT 优化
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 15

# Keepalive 优化
net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_keepalive_intvl = 10
net.ipv4.tcp_keepalive_probes = 6
EOF

sysctl -p
```

### JVM 跨中心调用优化

```java
// 跨中心调用客户端配置
@Configuration
public class CrossDCClientConfig {

    @Bean
    public WebClient crossDCWebClient() {
        // 低时延 HTTP 客户端
        HttpClient httpClient = HttpClient.create()
            .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000)
            .option(ChannelOption.TCP_NODELAY, true)
            .option(ChannelOption.SO_KEEPALIVE, true)
            .responseTimeout(Duration.ofSeconds(10))
            .doOnConnected(conn ->
                conn.addHandlerLast(new ReadTimeoutHandler(10))
                    .addHandlerLast(new WriteTimeoutHandler(10))
            );

        return WebClient.builder()
            .clientConnector(new ReactorClientHttpConnector(httpClient))
            .build();
    }

    @Bean
    public ConnectionPoolingClientConnectionManager connectionManager() {
        // 连接池配置
        PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager();
        cm.setMaxTotal(200);
        cm.setDefaultMaxPerRoute(50);
        cm.setValidateAfterInactivity(1000);
        return cm;
    }
}
```

---

## 总结

### 两地三中心关键指标

| 指标 | 目标值 | 监控方式 |
|------|-------|---------|
| 同城切换 RTO | < 30s | Patroni + 脚本 |
| 异地切换 RTO | < 5min | 自动化脚本 |
| 同城复制 RPO | 0 | 同步复制 |
| 异地复制 RPO | < 1min | 异步复制 |
| 跨中心网络延迟 | 同城 < 2ms, 异地 < 30ms | Blackbox Exporter |
| 服务可用性 | 99.99% | Prometheus SLA |

### 检查清单

- [ ] GSLB 配置并测试健康检查
- [ ] 数据库同步复制配置验证
- [ ] 消息队列 MirrorMaker 部署
- [ ] 自动故障切换脚本测试
- [ ] 监控告警规则配置
- [ ] 定期演练计划制定
- [ ] 混沌工程测试通过
- [ ] 网络专线冗余验证
- [ ] 文档和 Runbook 完善

### 架构演进路线

1. **阶段一**: 完成基础两地三中心部署
2. **阶段二**: 实现自动化故障切换
3. **阶段三**: 混沌工程常态化
4. **阶段四**: 多活架构演进